{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "132970a6-1c1f-48de-b910-4ab0d73a16da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Transactions to S3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a877a5be-c852-4c65-9e08-615518cdba77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.databricks.env.AWS_ACCESS_KEY\", \"your-access-key\")\n",
    "# spark.conf.set(\"spark.databricks.env.AWS_SECRET_KEY\", \"your-secret-key\")\n",
    "# spark.conf.set(\"spark.databricks.env.S3_BUCKET_NAME\", \"your-bucket-name\")\n",
    "# spark.conf.set(\"spark.databricks.env.GDRIVE_FILE_ID\", \"your-gdrive-id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e75be0e5-e6b1-4ce4-b050-6d3ccf27c3bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'default.transactions_data_checkpoint' already exists.\nUploaded 10000 records to transaction_data/transactions_20250610_115542.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115555.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115611.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115625.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115641.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115655.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115709.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115724.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115740.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115756.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115809.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115823.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115836.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115851.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115907.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115922.csv\nUploaded 10000 records to transaction_data/transactions_20250610_115955.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120012.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120026.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120043.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120056.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120111.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120125.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120140.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120156.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120210.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120223.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120236.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120252.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120309.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120324.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120338.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120351.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120404.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120423.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120436.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120454.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120507.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120521.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120535.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120555.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120609.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120623.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120637.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120653.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120706.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120722.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120738.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120754.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120812.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120825.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120839.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120854.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120909.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120925.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120939.csv\nUploaded 10000 records to transaction_data/transactions_20250610_120953.csv\nUploaded 10000 records to transaction_data/transactions_20250610_121008.csv\nUploaded 4643 records to transaction_data/transactions_20250610_121034.csv\nNo new data.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LoadTransactions\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "AWS_ACCESS_KEY = spark.conf.get(\"spark.databricks.env.AWS_ACCESS_KEY\")\n",
    "AWS_SECRET_KEY = spark.conf.get(\"spark.databricks.env.AWS_SECRET_KEY\")\n",
    "S3_BUCKET_NAME = spark.conf.get(\"spark.databricks.env.S3_BUCKET_NAME\")\n",
    "GDRIVE_FILE_ID = spark.conf.get(\"spark.databricks.env.GDRIVE_FILE_ID\")\n",
    "\n",
    "\n",
    "# S3 Client\n",
    "s3 = boto3.client(\"s3\", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)\n",
    "\n",
    "# GDrive URL\n",
    "file_id = GDRIVE_FILE_ID.split(\"/\")[-2]\n",
    "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "\n",
    "BATCH_SIZE = 10000\n",
    "\n",
    "\n",
    "CHECKPOINT_TABLE = \"default.transactions_data_checkpoint\"\n",
    "\n",
    "try:\n",
    "    spark.table(CHECKPOINT_TABLE)  # Try accessing the table\n",
    "    print(f\"Table '{CHECKPOINT_TABLE}' already exists.\")\n",
    "except AnalysisException:\n",
    "    print(f\"Table '{CHECKPOINT_TABLE}' does not exist. Creating now...\")\n",
    "\n",
    "    # Create the Delta table with one row containing 0\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE  {CHECKPOINT_TABLE} (\n",
    "            id INT\n",
    "        ) USING DELTA\n",
    "        LOCATION 'dbfs:/user/hive/warehouse/transactions_data_checkpoint'\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert the initial value\n",
    "    spark.sql(f\"INSERT INTO {CHECKPOINT_TABLE} VALUES (0)\")\n",
    "\n",
    "\n",
    "# Process Chunk\n",
    "def process_next_chunk():\n",
    "    while True:\n",
    "        df = pd.read_csv(download_url)\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "\n",
    "        last_idx = spark.sql(f\"SELECT max(id) FROM {CHECKPOINT_TABLE}\").collect()[0][0]\n",
    "        chunk = df.iloc[last_idx:last_idx + BATCH_SIZE]\n",
    "        if chunk.empty:\n",
    "            print(\"No new data.\")\n",
    "            return\n",
    "\n",
    "        # Upload chunk to S3\n",
    "        csv_buf = io.StringIO()\n",
    "        chunk.to_csv(csv_buf, index=False)\n",
    "        file_key = f\"transaction_data/transactions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        s3.put_object(Bucket=S3_BUCKET_NAME, Key=file_key, Body=csv_buf.getvalue())\n",
    "\n",
    "        # Update checkpoint\n",
    "        spark.sql(f\"DELETE FROM {CHECKPOINT_TABLE}\")\n",
    "        spark.sql(f\"INSERT INTO {CHECKPOINT_TABLE} VALUES ({last_idx + len(chunk)})\")\n",
    "        print(f\"Uploaded {len(chunk)} records to {file_key}\") \n",
    "process_next_chunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d562f293-8d86-4804-8997-43a7338fc6ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Customer Pattern Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cceb1fb9-b2aa-44ba-8c59-871f772d721b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern detection completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import io\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, lit, when\n",
    "from pyspark.sql.functions import percentile_approx\n",
    "\n",
    "# ---------- SETUP ----------\n",
    "\n",
    "# Load AWS credentials from env\n",
    "AWS_ACCESS_KEY = spark.conf.get(\"spark.databricks.env.AWS_ACCESS_KEY\")\n",
    "AWS_SECRET_KEY = spark.conf.get(\"spark.databricks.env.AWS_SECRET_KEY\")\n",
    "S3_BUCKET= spark.conf.get(\"spark.databricks.env.S3_BUCKET_NAME\")\n",
    "# IST timezone\n",
    "IST = pytz.timezone('Asia/Kolkata')\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MechanismY_PatternDetection\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "# AWS S3 boto3 client\n",
    "s3_client = boto3.client(\"s3\", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)\n",
    "\n",
    "# Paths\n",
    "TRANSACTION_PATH = f\"s3a://{S3_BUCKET}/transaction_data/\"\n",
    "IMPORTANCE_PATH = f\"s3a://{S3_BUCKET}/customer_importance/\"\n",
    "OUTPUT_PATH = f\"s3a://{S3_BUCKET}/detections/\"\n",
    "\n",
    "# ---------- READ DATA ----------\n",
    "\n",
    "transactions_df = spark.read.option(\"header\", \"true\").csv(TRANSACTION_PATH)\n",
    "transactions_df = transactions_df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "\n",
    "importance_df = spark.read.option(\"header\", \"true\").csv(IMPORTANCE_PATH)\n",
    "importance_df = importance_df.withColumn(\"Weight\", col(\"Weight\").cast(\"double\"))\n",
    "\n",
    "# ---------- MERGE DATA ----------\n",
    "\n",
    "merged_df = transactions_df.join(importance_df,\n",
    "    (transactions_df[\"customer\"] == importance_df[\"Source\"]) &\n",
    "    (transactions_df[\"merchant\"] == importance_df[\"Target\"]), \"left\"\n",
    ").select(transactions_df[\"*\"], importance_df[\"Weight\"])\n",
    "\n",
    "\n",
    "now_ist = datetime.now(IST).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# ---------- PATTERN 1: ----------\n",
    "\n",
    "agg_df = merged_df.groupBy(\"merchant\", \"customer\") \\\n",
    "    .agg(count(\"*\").alias(\"total_txns\"), avg(\"Weight\").alias(\"avg_weight\"))\n",
    "\n",
    "merchant_txn_counts = merged_df.groupBy(\"merchant\") \\\n",
    "    .agg(count(\"*\").alias(\"merchant_txns\"))\n",
    "\n",
    "percentiles_df = agg_df.groupBy(\"merchant\").agg(\n",
    "    percentile_approx(\"total_txns\", 0.99).alias(\"txn_99\"),\n",
    "    percentile_approx(\"avg_weight\", 0.01).alias(\"weight_01\")\n",
    ")\n",
    "\n",
    "pat1_df = agg_df.join(percentiles_df, \"merchant\") \\\n",
    "    .join(merchant_txn_counts, \"merchant\") \\\n",
    "    .filter(\n",
    "        (col(\"merchant_txns\") > 50000) &\n",
    "        (col(\"total_txns\") >= col(\"txn_99\")) &\n",
    "        (col(\"avg_weight\") <= col(\"weight_01\"))\n",
    "    ).selectExpr(\n",
    "        f\"'{now_ist}' as YStartTime\",\n",
    "        f\"'{now_ist}' as detectionTime\",\n",
    "        \"'PatId1' as patternId\",\n",
    "        \"'UPGRADE' as ActionType\",\n",
    "        \"customer as CustomerName\",\n",
    "        \"merchant as MerchantId\"\n",
    "    )\n",
    "\n",
    "# ---------- PATTERN 2: CHILD ----------\n",
    "\n",
    "pat2_df = merged_df.groupBy(\"merchant\", \"customer\") \\\n",
    "    .agg(avg(\"amount\").alias(\"avg_amt\"), count(\"*\").alias(\"total_txns\")) \\\n",
    "    .filter((col(\"avg_amt\") < 23) & (col(\"total_txns\") >= 80)) \\\n",
    "    .selectExpr(\n",
    "        f\"'{now_ist}' as YStartTime\",\n",
    "        f\"'{now_ist}' as detectionTime\",\n",
    "        \"'PatId2' as patternId\",\n",
    "        \"'CHILD' as ActionType\",\n",
    "        \"customer as CustomerName\",\n",
    "        \"merchant as MerchantId\"\n",
    "    )\n",
    "\n",
    "# ---------- PATTERN 3: DEI-NEEDED ----------\n",
    "\n",
    "gender_stats = transactions_df.groupBy(\"merchant\", \"gender\") \\\n",
    "    .agg(count(\"customer\").alias(\"gender_count\"))\n",
    "\n",
    "# Pivot to get male/female counts per merchant\n",
    "gender_pivot = gender_stats.groupBy(\"merchant\").pivot(\"gender\", [\"Male\", \"Female\"]).sum(\"gender_count\") \\\n",
    "    .na.fill(0)\n",
    "\n",
    "pat3_df = gender_pivot.filter(\n",
    "    (col(\"Female\") > 100) & (col(\"Female\") < col(\"Male\"))\n",
    ").selectExpr(\n",
    "    f\"'{now_ist}' as YStartTime\",\n",
    "    f\"'{now_ist}' as detectionTime\",\n",
    "    \"'PatId3' as patternId\",\n",
    "    \"'DEI-NEEDED' as ActionType\",\n",
    "    \"'' as CustomerName\",\n",
    "    \"merchant as MerchantId\"\n",
    ")\n",
    "\n",
    "# ---------- COMBINE & BATCH ----------\n",
    "\n",
    "all_detections = pat1_df.union(pat2_df).union(pat3_df)\n",
    "detection_rows = all_detections.collect()\n",
    "\n",
    "batch_size = 50\n",
    "for i in range(0, len(detection_rows), batch_size):\n",
    "    batch = detection_rows[i:i+batch_size]\n",
    "    batch_df = spark.createDataFrame(batch)\n",
    "\n",
    "    # Convert to CSV\n",
    "    csv_buffer = io.StringIO()\n",
    "    batch_df.toPandas().to_csv(csv_buffer, index=False)\n",
    "\n",
    "    filename = f\"detection_{int(time.time())}_{i//batch_size}.csv\"\n",
    "    s3_client.put_object(Bucket=S3_BUCKET, Key=f\"detections/{filename}\", Body=csv_buffer.getvalue())\n",
    "    # print(f\"Uploaded: {filename} [{len(batch)} records]\")\n",
    "\n",
    "print(\"Pattern detection completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bank_Transaction_Data_Pattern_Analaysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}