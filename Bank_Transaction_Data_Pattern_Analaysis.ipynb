{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "132970a6-1c1f-48de-b910-4ab0d73a16da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Transactions to S3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e75be0e5-e6b1-4ce4-b050-6d3ccf27c3bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 10000 records to transaction_data/transactions_20250609_181436.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LoadTransactions\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Environment Variables\n",
    "AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "S3_BUCKET_NAME = os.getenv(\"S3_BUCKET_NAME\")\n",
    "GDRIVE_FILE_ID = os.getenv(\"GDRIVE_FILE_ID\")\n",
    "\n",
    "# S3 Client\n",
    "s3 = boto3.client(\"s3\", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)\n",
    "\n",
    "# GDrive URL\n",
    "file_id = GDRIVE_FILE_ID.split(\"/\")[-2]\n",
    "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Checkpoint\n",
    "CHECKPOINT_TABLE = \"default.transaction_checkpoint\"\n",
    "BATCH_SIZE = 10000\n",
    "\n",
    "# Ensure Delta Table\n",
    "if not DeltaTable.isDeltaTable(spark, f\"/user/hive/warehouse/{CHECKPOINT_TABLE}\"):\n",
    "    spark.sql(f\"CREATE TABLE {CHECKPOINT_TABLE} (last_index INT) USING DELTA\")\n",
    "    spark.sql(f\"INSERT INTO {CHECKPOINT_TABLE} VALUES (0)\")\n",
    "\n",
    "# Process Chunk\n",
    "def process_next_chunk():\n",
    "    df = pd.read_csv(download_url)\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "\n",
    "    last_idx = spark.sql(f\"SELECT max(last_index) FROM {CHECKPOINT_TABLE}\").collect()[0][0]\n",
    "    chunk = df.iloc[last_idx:last_idx + BATCH_SIZE]\n",
    "    if chunk.empty:\n",
    "        print(\"No new data.\")\n",
    "        return\n",
    "\n",
    "    # Upload chunk to S3\n",
    "    csv_buf = io.StringIO()\n",
    "    chunk.to_csv(csv_buf, index=False)\n",
    "    file_key = f\"transaction_data/transactions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    s3.put_object(Bucket=S3_BUCKET_NAME, Key=file_key, Body=csv_buf.getvalue())\n",
    "\n",
    "    # Update checkpoint\n",
    "    spark.sql(f\"DELETE FROM {CHECKPOINT_TABLE}\")\n",
    "    spark.sql(f\"INSERT INTO {CHECKPOINT_TABLE} VALUES ({last_idx + len(chunk)})\")\n",
    "    print(f\"Uploaded {len(chunk)} records to {file_key}\")\n",
    "    return \n",
    "# Call periodically (every sec if scheduled)\n",
    "process_next_chunk()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d562f293-8d86-4804-8997-43a7338fc6ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Customer Pattern Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cceb1fb9-b2aa-44ba-8c59-871f772d721b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Uploaded: detection_1749495410_0.csv [50 records]\nâœ… Uploaded: detection_1749495412_1.csv [50 records]\nâœ… Uploaded: detection_1749495413_2.csv [50 records]\nâœ… Uploaded: detection_1749495414_3.csv [50 records]\nâœ… Uploaded: detection_1749495415_4.csv [50 records]\nâœ… Uploaded: detection_1749495416_5.csv [50 records]\nâœ… Uploaded: detection_1749495416_6.csv [50 records]\nâœ… Uploaded: detection_1749495417_7.csv [50 records]\nâœ… Uploaded: detection_1749495418_8.csv [50 records]\nâœ… Uploaded: detection_1749495419_9.csv [50 records]\nâœ… Uploaded: detection_1749495420_10.csv [50 records]\nâœ… Uploaded: detection_1749495421_11.csv [50 records]\nâœ… Uploaded: detection_1749495422_12.csv [50 records]\nâœ… Uploaded: detection_1749495422_13.csv [50 records]\nâœ… Uploaded: detection_1749495423_14.csv [33 records]\nðŸŽ¯ Pattern detection completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import io\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, lit, when\n",
    "from pyspark.sql.functions import percentile_approx\n",
    "\n",
    "# ---------- SETUP ----------\n",
    "\n",
    "# Load AWS credentials from env\n",
    "AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "S3_BUCKET = os.getenv(\"S3_BUCKET_NAME\")\n",
    "\n",
    "# IST timezone\n",
    "IST = pytz.timezone('Asia/Kolkata')\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MechanismY_PatternDetection\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "# AWS S3 boto3 client\n",
    "s3_client = boto3.client(\"s3\", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY)\n",
    "\n",
    "# Paths\n",
    "TRANSACTION_PATH = f\"s3a://{S3_BUCKET}/transaction_data/\"\n",
    "IMPORTANCE_PATH = f\"s3a://{S3_BUCKET}/customer_importance/\"\n",
    "OUTPUT_PATH = f\"s3a://{S3_BUCKET}/detections/\"\n",
    "\n",
    "# ---------- READ DATA ----------\n",
    "\n",
    "transactions_df = spark.read.option(\"header\", \"true\").csv(TRANSACTION_PATH)\n",
    "transactions_df = transactions_df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "\n",
    "importance_df = spark.read.option(\"header\", \"true\").csv(IMPORTANCE_PATH)\n",
    "importance_df = importance_df.withColumn(\"Weight\", col(\"Weight\").cast(\"double\"))\n",
    "\n",
    "# ---------- MERGE DATA ----------\n",
    "\n",
    "merged_df = transactions_df.join(importance_df,\n",
    "    (transactions_df[\"customer\"] == importance_df[\"Source\"]) &\n",
    "    (transactions_df[\"merchant\"] == importance_df[\"Target\"]), \"left\"\n",
    ").select(transactions_df[\"*\"], importance_df[\"Weight\"])\n",
    "\n",
    "# ---------- YStartTime / DetectionTime ----------\n",
    "\n",
    "now_ist = datetime.now(IST).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# ---------- PATTERN 1: UPGRADE ----------\n",
    "\n",
    "agg_df = merged_df.groupBy(\"merchant\", \"customer\") \\\n",
    "    .agg(count(\"*\").alias(\"total_txns\"), avg(\"Weight\").alias(\"avg_weight\"))\n",
    "\n",
    "merchant_txn_counts = merged_df.groupBy(\"merchant\") \\\n",
    "    .agg(count(\"*\").alias(\"merchant_txns\"))\n",
    "\n",
    "percentiles_df = agg_df.groupBy(\"merchant\").agg(\n",
    "    percentile_approx(\"total_txns\", 0.99).alias(\"txn_99\"),\n",
    "    percentile_approx(\"avg_weight\", 0.01).alias(\"weight_01\")\n",
    ")\n",
    "\n",
    "pat1_df = agg_df.join(percentiles_df, \"merchant\") \\\n",
    "    .join(merchant_txn_counts, \"merchant\") \\\n",
    "    .filter(\n",
    "        (col(\"merchant_txns\") > 50000) &\n",
    "        (col(\"total_txns\") >= col(\"txn_99\")) &\n",
    "        (col(\"avg_weight\") <= col(\"weight_01\"))\n",
    "    ).selectExpr(\n",
    "        f\"'{now_ist}' as YStartTime\",\n",
    "        f\"'{now_ist}' as detectionTime\",\n",
    "        \"'PatId1' as patternId\",\n",
    "        \"'UPGRADE' as ActionType\",\n",
    "        \"customer as CustomerName\",\n",
    "        \"merchant as MerchantId\"\n",
    "    )\n",
    "\n",
    "# ---------- PATTERN 2: CHILD ----------\n",
    "\n",
    "pat2_df = merged_df.groupBy(\"merchant\", \"customer\") \\\n",
    "    .agg(avg(\"amount\").alias(\"avg_amt\"), count(\"*\").alias(\"total_txns\")) \\\n",
    "    .filter((col(\"avg_amt\") < 23) & (col(\"total_txns\") >= 80)) \\\n",
    "    .selectExpr(\n",
    "        f\"'{now_ist}' as YStartTime\",\n",
    "        f\"'{now_ist}' as detectionTime\",\n",
    "        \"'PatId2' as patternId\",\n",
    "        \"'CHILD' as ActionType\",\n",
    "        \"customer as CustomerName\",\n",
    "        \"merchant as MerchantId\"\n",
    "    )\n",
    "\n",
    "# ---------- PATTERN 3: DEI-NEEDED ----------\n",
    "\n",
    "gender_stats = transactions_df.groupBy(\"merchant\", \"gender\") \\\n",
    "    .agg(count(\"customer\").alias(\"gender_count\"))\n",
    "\n",
    "# Pivot to get male/female counts per merchant\n",
    "gender_pivot = gender_stats.groupBy(\"merchant\").pivot(\"gender\", [\"Male\", \"Female\"]).sum(\"gender_count\") \\\n",
    "    .na.fill(0)\n",
    "\n",
    "pat3_df = gender_pivot.filter(\n",
    "    (col(\"Female\") > 100) & (col(\"Female\") < col(\"Male\"))\n",
    ").selectExpr(\n",
    "    f\"'{now_ist}' as YStartTime\",\n",
    "    f\"'{now_ist}' as detectionTime\",\n",
    "    \"'PatId3' as patternId\",\n",
    "    \"'DEI-NEEDED' as ActionType\",\n",
    "    \"'' as CustomerName\",\n",
    "    \"merchant as MerchantId\"\n",
    ")\n",
    "\n",
    "# ---------- COMBINE & BATCH ----------\n",
    "\n",
    "all_detections = pat1_df.union(pat2_df).union(pat3_df)\n",
    "detection_rows = all_detections.collect()\n",
    "\n",
    "batch_size = 50\n",
    "for i in range(0, len(detection_rows), batch_size):\n",
    "    batch = detection_rows[i:i+batch_size]\n",
    "    batch_df = spark.createDataFrame(batch)\n",
    "\n",
    "    # Convert to CSV\n",
    "    csv_buffer = io.StringIO()\n",
    "    batch_df.toPandas().to_csv(csv_buffer, index=False)\n",
    "\n",
    "    filename = f\"detection_{int(time.time())}_{i//batch_size}.csv\"\n",
    "    s3_client.put_object(Bucket=S3_BUCKET, Key=f\"detections/{filename}\", Body=csv_buffer.getvalue())\n",
    "    print(f\"âœ… Uploaded: {filename} [{len(batch)} records]\")\n",
    "\n",
    "print(\"ðŸŽ¯ Pattern detection completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bank_Transaction_Data_Pattern_Analaysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}